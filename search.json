[
    
    
    {
      "objectID": "Assingment 4.html",
      "href": "Assingment 4.html",
      "title": "Assignment 4",
      "section": "",
      "text": "In this article, I will do some PCA (Principal Component) and clustering analysis on U.S. crime data.\nWith PCA we try to find permutations of a data-set that identifies what relationships are resposible for causing the most uncorrelated variance. Specifically, this transformation is done for ‘dimensions’ of the data by rotating the original data (making it orthogonal to and hence uncorrelated like it was in the original) along a new axis in a way that minimizes the average (squared) distance between observations and the new direction so taken. Often, you can work with the first two dimensions that will permit this type of transformation to make comparisons about your data-set.\nSo, I first load in my data-set:\n\nlibrary(datasets)\nlibrary(ISLR)\narrest = USArrests\nstates = row.names(USArrests)\nnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\n\nI get the means\n\napply(USArrests, 2, mean)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\nI also grab the variations\n\napply(USArrests, 2, var)\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n\nI then apply PCA with scaling\n\npr.out = prcomp(USArrests, scale = TRUE)\nnames(pr.out)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nThen I center the data\n\npr.out$center\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\nI make a ‘variable loadings matrix’\n\npr.out$scale\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\n\nthen rotate it\n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n\ncheck dimension size\n\ndim(pr.out$x)\n\n[1] 50  4\n\n\nFinally, outputting:\n\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\n\n\n\ncompute the standard deviation\n\npr.out$sdev\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\n\ncalculate the variance by squaring and specifiy the proportion of variance explained\n\npr.var=pr.out$sdev^2\npve=pr.var/sum(pr.var)\n\nplotting:\n\nplot(pve, xlab=\"Principla Component\", ylab = \"Proportion of Variance Explained\", ylim=c(0,1),type = 'b')\n\n\n\n\nto show the cumulative effect of explanatory power in regard to variance, I use:\n\nplot(cumsum(pve), xlab=\"Principal Component\", ylab=\"Cumulative Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n\n\n\nFinally, plotting out:\n\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_pca_biplot(pr.out, font.family = \"Georgia\", col.var = \"firebrick1\")\n\n\n\n\nNext, I move on to K-Means clustering.\n\nThe goal of K-Means is to find the amount of clusters of observations within the data the minimizes the sum of sqaures within those clusters. Of course, we can alos sepcify the number of clusters as well. First, I load in my new data-set, perform other set-up steps and chart the difference between hard-drive size and ram capacity in each PC:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(animation)\ncomputers = read.csv(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv\")\n\nrescaled_comp &lt;- computers[4:5] %&gt;%\n  mutate(hd_scal = scale(hd),\n         ram_scal = scale(ram)) %&gt;%\n  select(c(hd_scal, ram_scal))\n  \nggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +\n  geom_point(pch=20, col = \"blue\") + theme_bw() +\n  labs(x = \"Hard drive size (Scaled)\", y =\"RAM size (Scaled)\" ) +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\nNext, I produce the output of the k-means process via an animation with (producing the output as a sequence of frames):\n\nset.seed(2345)\nkmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, I try the k-means approach with some flower data via the ‘Iris’ data-set:\n\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n\nHere, we can see the result of grouping the real data by flower species. Now, setting up our algorithm:\n\nset.seed(20)\nirisCluster &lt;- kmeans(iris[, 3:4], 3, nstart = 20)\nirisCluster\n\nK-means clustering with 3 clusters of sizes 50, 48, 52\n\nCluster means:\n  Petal.Length Petal.Width\n1     1.462000    0.246000\n2     5.595833    2.037500\n3     4.269231    1.342308\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\nWithin cluster sum of squares by cluster:\n[1]  2.02200 16.29167 13.05769\n (between_SS / total_SS =  94.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nChecking the class:\n\nclass(irisCluster$cluster)\n\n[1] \"integer\"\n\n\nand comparing the successfullness of the k-means approach with a confusion matrix:\n\ntable(irisCluster$cluster, iris$Species)\n\n   \n    setosa versicolor virginica\n  1     50          0         0\n  2      0          2        46\n  3      0         48         4\n\n\nFinally, outputting the graphical results of the k-means approach compared with the real data:\n\nlibrary(grid)\nlibrary(gridExtra)\nirisCluster$cluster &lt;- as.factor(irisCluster$cluster)\nactual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nkmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\", \"darkblue\", \"forestgreen\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \ngrid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)\n\n\n\n\nFinally, let’s try some wine…or at least an analysis of some wine data.\n\nSetting up:\n\nlibrary(readr)\nwine &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/gentlemachinelearning/master/data/wine.csv\")\nwine_subset &lt;- scale(wine[ , c(2:4)])\nwine_cluster &lt;- kmeans(wine_subset, centers = 3,\n                       iter.max = 10,\n                       nstart = 25)\nwine_cluster\n\nK-means clustering with 3 clusters of sizes 48, 60, 70\n\nCluster means:\n     Alcohol      Malic        Ash\n1  0.1470536  1.3907328  0.2534220\n2  0.8914655 -0.4522073  0.5406223\n3 -0.8649501 -0.5660390 -0.6371656\n\nClustering vector:\n  [1] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n [38] 2 3 1 2 1 2 1 3 1 1 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 2 3 3 2 2 2\n [75] 3 3 3 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 1 3 3 3 3 3 1 3 3 2 1 1 1 3 3 3 3 1 3 1 3 1 3 3 1 1 1 1 1 2 1 1 1 1 1 1\n[149] 1 1 1 1 2 1 3 1 1 1 2 2 1 1 1 1 2 1 1 1 2 1 3 3 2 1 1 1 2 1\n\nWithin cluster sum of squares by cluster:\n[1]  73.71460  67.98619 111.63512\n (between_SS / total_SS =  52.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nPlotting within-cluster sum of squares\n\nwssplot &lt;- function(data, nc=15, seed=1234){\n  wss &lt;- (nrow(data)-1)*sum(apply(data,2,var))\n  for (i in 2:nc){\n    set.seed(seed)\n    wss[i] &lt;- sum(kmeans(data, centers=i)$withinss)}\n  plot(1:nc, wss, type=\"b\", xlab=\"Number of Clusters\",\n       ylab=\"Within groups sum of squares\")\n}\nwssplot(wine_subset, nc = 9)\n\n\n\n\nNow, to plot by dimensions:\n\nwine_cluster$cluster = as.factor(wine_cluster$cluster)\npairs(wine[2:4],\n      col = c(\"firebrick1\", \"darkblue\", \"forestgreen\")[wine_cluster$cluster],\n      pch = c(15:17)[wine_cluster$cluster],\n      main = \"K-Means Clusters: Wine data\")\n\n\n\n\nInspect:\n\ntable(wine_cluster$cluster)\n\n\n 1  2  3 \n48 60 70 \n\n\nGraph impact of clusters #s:\n\nlibrary(factoextra)\nfviz_nbclust(wine_subset, kmeans, method = \"wss\")\n\n\n\n\nFancifully:\n\nwine.km &lt;- eclust(wine_subset, \"kmeans\", nboot = 2)\n\n\n\n\nPrint output of doing k-means via ‘eclust’:\n\nwine.km\n\nK-means clustering with 3 clusters of sizes 60, 70, 48\n\nCluster means:\n     Alcohol      Malic        Ash\n1  0.8914655 -0.4522073  0.5406223\n2 -0.8649501 -0.5660390 -0.6371656\n3  0.1470536  1.3907328  0.2534220\n\nClustering vector:\n  [1] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n [38] 1 2 3 1 3 1 3 2 3 3 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 2 2 1 1 1\n [75] 2 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 3 2 2 2 2 2 3 2 2 1 3 3 3 2 2 2 2 3 2 3 2 3 2 2 3 3 3 3 3 1 3 3 3 3 3 3\n[149] 3 3 3 3 1 3 2 3 3 3 1 1 3 3 3 3 1 3 3 3 1 3 2 2 1 3 3 3 1 3\n\nWithin cluster sum of squares by cluster:\n[1]  67.98619 111.63512  73.71460\n (between_SS / total_SS =  52.3 %)\n\nAvailable components:\n\n [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"       \"clust_plot\"  \n[11] \"silinfo\"      \"nbclust\"      \"data\"         \"gap_stat\"    \n\n\nFind the optimal # of clusters using gap stats:\n\nwine.km$nbclust\n\n[1] 3\n\n\nGraph the optimality range:\n\nfviz_nbclust(wine_subset, kmeans, method = \"gap_stat\")\n\n\n\n\nMake a silhouette plot\n\nfviz_silhouette(wine.km)\n\n  cluster size ave.sil.width\n1       1   60          0.44\n2       2   70          0.33\n3       3   48          0.30\n\n\n\n\n\nFinally:\n\nfviz_cluster(wine_cluster, data = wine_subset, ellipse.type = \"norm\") + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\nAs a little bonus, we will do some hierarchical clustering with the crime data we started with.\n\nThe purpose of such clustering is to find the attribute of observations that have the smallest ‘eludican distance’ from one another, measured along some dimension of the data. Then build up from that with the next closest pairing, etc (although there are other possible building methods one could use). We first run the algorithm after scaling all variables and computing their ‘euclidean distance’ from each other along various dimensions, and then generate a dendrogram of the output:\n\nlibrary(cluster)\narrest.hc &lt;- USArrests %&gt;%\n  scale() %&gt;%                    \n  dist(method = \"euclidean\") %&gt;% \n  hclust(method = \"ward.D2\") \n\nfviz_dend(arrest.hc, k = 4, # Four groups\n          cex = 0.5, \n          k_colors = c(\"firebrick1\",\"forestgreen\",\"blue\", \"purple\"),\n          color_labels_by_k = TRUE, # color labels by groups\n          rect = TRUE, # Add rectangle (cluster) around groups,\n          main = \"Cluster Dendrogram: USA Arrest data\"\n) + theme(text = element_text(family=\"Georgia\")) \n\n\n\n\nTa-da! We can see certain states are found the be more ‘natural’ co-habitors via this process than others (of course, we did specify for the algorithm to break all the data down to four groups). Observations are most similar to other observations with the same height. The greater the height difference the greater the dissimilarity. From here we can try, for instance, to see how other variables of states within the same cluster relate to each other."
    },
    {
      "objectID": "Ass 6.html",
      "href": "Ass 6.html",
      "title": "Assignment 6",
      "section": "",
      "text": "Setting up our data-sets:\n\nlibrary(easypackages)\nlibraries(\"arm\",\"MASS\",\"ISLR\")\nattach(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n\nPlot:\n\nplot(medv~lstat,Boston, pch=20, cex=.8, col=\"steelblue\")\n\n\n\n\nthen fit a new simple OLS line with explanatory variable ‘medv’\n\nfit1=lm(medv~lstat,data=Boston)\nfit1\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\n\n\nsummary(fit1)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\nnames(fit1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nMake predictions using the lstat variable:\n\npredict(fit1,data.frame(lstat=c(0,5,10,15)),interval=\"confidence\")\n\n       fit      lwr      upr\n1 34.55384 33.44846 35.65922\n2 29.80359 29.00741 30.59978\n3 25.05335 24.47413 25.63256\n4 20.30310 19.73159 20.87461\n\n\n\npredict(fit1,data.frame(lstat=c(0,5,10,15)),interval=\"prediction\")\n\n       fit       lwr      upr\n1 34.55384 22.291923 46.81576\n2 29.80359 17.565675 42.04151\n3 25.05335 12.827626 37.27907\n4 20.30310  8.077742 32.52846\n\n\nLet’s try some multi-variate regression:\n\nfit2=lm(medv~lstat+age,data=Boston)\nsummary(fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\n\nfit3=lm(medv~.,Boston)\nsummary(fit3)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nThis will include all variables except Boston\n\npar(mfrow=c(2,2))\nplot(fit3,pch=20, cex=.8, col=\"steelblue\")\nmtext(\"fit3\", side = 3, line = - 2, cex = 2, outer = TRUE)\n\n\n\n\nTry another permutation:\n\nfit4=update(fit3,~.-age-indus)\nsummary(fit4)\n\n\nCall:\nlm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + black + lstat, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***\ncrim         -0.108413   0.032779  -3.307 0.001010 ** \nzn            0.045845   0.013523   3.390 0.000754 ***\nchas          2.718716   0.854240   3.183 0.001551 ** \nnox         -17.376023   3.535243  -4.915 1.21e-06 ***\nrm            3.801579   0.406316   9.356  &lt; 2e-16 ***\ndis          -1.492711   0.185731  -8.037 6.84e-15 ***\nrad           0.299608   0.063402   4.726 3.00e-06 ***\ntax          -0.011778   0.003372  -3.493 0.000521 ***\nptratio      -0.946525   0.129066  -7.334 9.24e-13 ***\nblack         0.009291   0.002674   3.475 0.000557 ***\nlstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nPlot some diagnostic stats:\n\npar(mfrow=c(2,2), main=\"fit4\")\nplot(fit4,pch=20, cex=.8, col=\"steelblue\")\nmtext(\"fit4\", side = 3, line = - 2, cex = 2, outer = TRUE)\n\n\n\n\nthen coefficients:\n\npar(mfrow=c(1,1))\narm::coefplot(fit4)\n\n\n\n\nLet us add some interaction terms:\n\nfit5=lm(medv~lstat*tax,Boston) \nsummary(fit5)\n\n\nCall:\nlm(formula = medv ~ lstat * tax, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.741  -4.108  -1.107   1.892  26.864 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 38.3760008  1.5944708  24.068  &lt; 2e-16 ***\nlstat       -1.0795139  0.1185942  -9.103  &lt; 2e-16 ***\ntax         -0.0116439  0.0040741  -2.858  0.00444 ** \nlstat:tax    0.0004416  0.0002460   1.795  0.07324 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.164 on 502 degrees of freedom\nMultiple R-squared:  0.5534,    Adjusted R-squared:  0.5508 \nF-statistic: 207.4 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\nanother\n\nfit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\n\npar(mfrow=c(1,1))\nplot(medv~lstat, pch=20, col=\"forestgreen\")\npoints(lstat,fitted(fit6),col=\"firebrick\",pch=20)\n\n\n\n\nadd a new fit to the graph:\n\npar(mfrow=c(1,1))\nplot(medv~lstat, pch=20, col=\"forestgreen\")\npoints(lstat,fitted(fit6),col=\"firebrick\",pch=20)\nfit7=lm(medv~poly(lstat,4))\npoints(lstat,fitted(fit7),col=\"steelblue\",pch=20)\n\n\n\n\nLet us take a crack at incorporating qualitative explanatory variables:\n\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\n\n\nsummary(Carseats)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\n\nAdd some interaction terms\n\nfit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)\n\n\nsummary(fit1)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Age:Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\n\nattach(Carseats)\ncontrasts(Carseats$ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nThis allows us to check on the comparative line-up of our factors, making sure they accord with out intuition or domain knowledge.\nLet us also make nice plot of ‘Price vs. Sales’ utilizing a range of functions:\n\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"firebrick\")\n}\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"steelblue\",pch=20)\n\n\n\n\nonto logit…\n\nWe are going to be analyzing the Taiwan Election and Democratization Study (TEDS) data-set. Specifically, we are going to use ‘votesai’, a vote for the female candidate of the then opposition party DPP, Tsai Ing-wen, as the dependent variable in a logistic regression.\nFirst, loading up and taking a first swing:\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nglm.vt=glm(votetsai~female,data=TEDS_2016,family=binomial)\nglm.vt\n\n\nCall:  glm(formula = votetsai ~ female, family = binomial, data = TEDS_2016)\n\nCoefficients:\n(Intercept)       female  \n    0.54971     -0.06517  \n\nDegrees of Freedom: 1260 Total (i.e. Null);  1259 Residual\n  (429 observations deleted due to missingness)\nNull Deviance:      1667 \nResidual Deviance: 1666     AIC: 1670\n\n\nIt look like females are sightly less likely to vote for Tsai Ing-wen. Specifically, when a respondent is female it appears the odds of voting for Tsai decreases by .94% on average (via the odds ratio exp(-0.06517)). We can try adding some more explanatory variables and see what happens:\n\nglm.vt2 &lt;- glm(\n  votetsai ~ female + KMT + DPP + age + Independence + Econ_worse + Govt_dont_care + Minnan_father + Mainland_father +   Taiwanese,\n  data=TEDS_2016,family=binomial\n)\nsummary(glm.vt2)\n\n\nCall:\nglm(formula = votetsai ~ female + KMT + DPP + age + Independence + \n    Econ_worse + Govt_dont_care + Minnan_father + Mainland_father + \n    Taiwanese, family = binomial, data = TEDS_2016)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.457408   0.400868  -1.141  0.25385    \nfemale          -0.085090   0.188540  -0.451  0.65177    \nKMT             -2.919937   0.257730 -11.329  &lt; 2e-16 ***\nDPP              2.480782   0.274907   9.024  &lt; 2e-16 ***\nage              0.008425   0.005786   1.456  0.14536    \nIndependence     1.010504   0.251712   4.015 5.96e-05 ***\nEcon_worse       0.309492   0.188410   1.643  0.10045    \nGovt_dont_care   0.003994   0.188244   0.021  0.98307    \nMinnan_father   -0.288656   0.251843  -1.146  0.25172    \nMainland_father -1.170440   0.387627  -3.020  0.00253 ** \nTaiwanese        0.938965   0.196506   4.778 1.77e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1666.53  on 1260  degrees of freedom\nResidual deviance:  769.46  on 1250  degrees of freedom\n  (429 observations deleted due to missingness)\nAIC: 791.46\n\nNumber of Fisher Scoring iterations: 6\n\n\nA much wider range of estimates is now available to see, indicating more complex relationships are probably operating beneath the surface.\nAs an extra check, we can run the ‘mrobust’ package in STATA in order to find the ‘best’ combination of predictor variables. I’ll skip over the coding stuff here, but the output that results from it is:\n\nWhich seems to imply that being ‘female’ may have less of on an impact on voting for ‘Tsai’ than suggested above."
    },
    {
        "objectID": "Lab1.html",
        "href": "Lab1.html",
        "title": "EPPS 6323: Lab01 R programming basics I",
        "section": "",
        "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"has_annotations\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9934483\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
      },
      {
        "objectID": "Lab1.html#create-object-using-the-assignment-operator--",
        "href": "Lab1.html#create-object-using-the-assignment-operator--",
        "title": "EPPS 6323: Lab01 R programming basics I",
        "section": "",
        "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
      },
      {
        "objectID": "Lab1.html#using-function",
        "href": "Lab1.html#using-function",
        "title": "EPPS 6323: Lab01 R programming basics I",
        "section": "",
        "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
      },
      {
        "objectID": "Lab1.html#using---operators",
        "href": "Lab1.html#using---operators",
        "title": "EPPS 6323: Lab01 R programming basics I",
        "section": "",
        "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"has_annotations\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
      },
      {
        "objectID": "Lab1.html#matrix-operations",
        "href": "Lab1.html#matrix-operations",
        "title": "EPPS 6323: Lab01 R programming basics I",
        "section": "",
        "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9934483\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
      },
      {
        "objectID": "Lab1.html#simple-descriptive-statistics-base",
        "href": "Lab1.html#simple-descriptive-statistics-base",
        "title": "EPPS 6323: Lab01 R programming basics I",
        "section": "",
        "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
      },
      {
        "objectID": "Lab1.html#visualization-using-r-graphics-without-packages",
        "href": "Lab01.html#visualization-using-r-graphics-without-packages",
        "title": "EPPS 6323: Lab01 R programming basics I",
        "section": "",
        "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
      },
      {
          "objectID": "Lab03.html",
          "href": "Lab03.html",
          "title": "EPPS 6323: Lab03 R programming (Exploratory Data Analysis)",
          "section": "R Programming (EDA)",
          "text": "R Programming (EDA)\n\n## Creating a function: regplot\n## Combine the lm, plot and abline functions to create a regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y)\n  abline(fit,col=\"red\")\n}\n\n\nattach(ISLR::Carseats)\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}  # \"...\" is called ellipsis, which is designed to take any number of named or unnamed arguments.\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n\n\n\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\nRegression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\nsummary(petal_lm)\n\n\nCall:\nlm(formula = Petal.Length ~ 0 + Sepal.Length + Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.70623 -0.51867 -0.08334  0.49844  1.93093 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nSepal.Length  1.56030    0.04557   34.24   &lt;2e-16 ***\nSepal.Width  -1.74570    0.08709  -20.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 148 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9726 \nF-statistic:  2663 on 2 and 148 DF,  p-value: &lt; 2.2e-16"
        },
    {
      "objectID": "Lab02.html",
      "href": "Lab02.html",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/n_/xgfchnwx5mg8m9v9kxbym5g40000gn/T//Rtmp92aHdq/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
    },
    {
      "objectID": "Lab02.html#indexing-data-using",
      "href": "Lab02.html#indexing-data-using",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
    },
    {
      "objectID": "Lab02.html#loading-data-from-github-remote",
      "href": "Lab02.html#loading-data-from-github-remote",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
    },
    {
      "objectID": "Lab02.html#load-data-from-islr-website",
      "href": "Lab02.html#load-data-from-islr-website",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
    },
    {
      "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
      "href": "Lab02.html#additional-graphical-and-numerical-summaries",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
    },
    {
      "objectID": "Lab02.html#linear-regression",
      "href": "Lab02.html#linear-regression",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/n_/xgfchnwx5mg8m9v9kxbym5g40000gn/T//Rtmp92aHdq/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
    },
    {
      "objectID": "Lab02.html#multiple-linear-regression",
      "href": "Lab02.html#multiple-linear-regression",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
    },
    {
      "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
      "href": "Lab02.html#non-linear-transformations-of-the-predictors",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
    },
    {
      "objectID": "Lab02.html#qualitative-predictors",
      "href": "Lab02.html#qualitative-predictors",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
    },
    {
      "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
      "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
      "title": "EPPS 6323: Lab02 R programming basics II",
      "section": "",
      "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
    },
    {
      "objectID": "Lab04.html",
      "href": "Lab04.html",
      "title": "EPPS 6323: Lab #4, R programming (Unsupervised learning)",
      "section": "",
      "text": "Unsupervised learning is a class of machine learning algorithms to identify patterns or grouping structure in the data. Unlike supervised learning which relies on “supervised” information such as the dependent variable to guide modeling, unsupervised learning seeks to explore the structure and possible groupings of unlabeled data. This information will be useful to provide pre-processor for supervised learning.\nUnsupervised learning has no explicit dependent variable of Y for prediction. Instead, the goal is to discover interesting patterns about the measurements on \\((X_{1}), (X_{2}), . . . , (X_{p})\\) and identify any subgroups among the observations.\nGenerally, in this section, the two general methods are introduced: Principal components analysis and Clustering.\n\n\nPrincipal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.\n\n\n\n\n\nThe K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
    },
    {
      "objectID": "Lab04.html#principal-component-analysis-pca",
      "href": "Lab04.html#principal-component-analysis-pca",
      "title": "EPPS 6323: Lab #4, R programming (Unsupervised learning)",
      "section": "",
      "text": "Principal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance."
    },
    {
      "objectID": "Lab04.html#clustering",
      "href": "Lab04.html#clustering",
      "title": "EPPS 6323: Lab #4, R programming (Unsupervised learning)",
      "section": "",
      "text": "The K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
    },
    {
      "objectID": "Assignment1.html",
      "href": "Assignment1.html",
      "title": "Assignment 1",
      "section": "",
      "text": "Statistics, when considered as a branch of reasoning, is occasionally harangued in the everyday world as being a worse base for forming conclusions than proverbial ‘lies, and damned lies’. Of course, the undertone of the sentiment is often to simply convey a gentle reminder about the dishonest (or at least incorrect) use of statistical tools to imply associations that are unsupportable on a sounder basis of application. A dichotomous lesson many first receive in an introductory statistics course while pursuing studies at an institution of higher education.\nOf course, the argument over the soundest basis for the use of statistical tools has a more rarefied pedigree within academia itself. Especially as regards their application in the social sciences. In this data-driven century, which comfortably weaves patterns of information out of the historically unparalleled petabyte range (Daoud and Dubhashi 2023, 27), what can best be conceptualized as schools of thought have emerged with competing understandings of the proper foundations and appropriate spheres of application of statistical methods (Daoud and Dubhashi, 2023). Two prominent instalments of this debate will be surveyed here. Namely, Leo Breiman’s 2001 article Statistical Modeling: The Two Cultures and Galit Shmueli’s 2010 paper To Explain or to Predict?.\nThe issue, as per usual, starts at the beginning: What is the purpose of doing statistics? The answer, according to Breiman, is to contribute to the advancement of science by allowing us to make predictions (Breiman 2001, 11–12). He mentions how in the private-sector, the only value of statistical analysis rests on its predictive accuracy (2001, 3-4). In other words, a statistic’s worth lies in the results it actuates. He also observes that, in contrast, academics involved in the pursuit of scientific knowledge typically value statistical tools for their ability to provide an interpretable, causal model of some phenomena under study (Breiman 2001, 4–5).1 Perhaps even ignoring matters of predictive strength (or lack thereof) in favour of “the construction of an ingenious stochastic model.” (2001, 5).\nNow, the epistemological underpinnings that Breiman is alluding to come clearly into the fore in his construal of why different practitioners value different criteria to adjudicate the ends of the scientific enterprise. Breiman sees the differing teleological visions of the two ‘camps’ of practitioners to be essentially conceptual instantiations of the properties of the tools they use to advance each other’s conception of knowledge (Breiman 2001, 1). Traditional statistical analysis often involves the use of linear parametric2 models that are presumed to represent the phenomena that are the origin of the data that is available to study (Breiman 2001, 1, 4–5, 6). These models are often chosen independently of the data under consideration and then tested against various goodness-of-fit measures (2001, 4). Breiman expresses credulity that anyone could seriously believe that such limited models, supposedly chosen because of their ease of manipulation and understanding (Breiman 2001, 4–6), would necessarily describe much of the complicated nature of reality (2001, 4-5). Here, he attacks the classical presumption of traditional modelling on its own terms. In essence, arguing that even if classical modelling’s focus on finding the ‘real’ nature of data-generating processes (Breiman 2001, 1) were the most fruitful goal of scientific inquiry, simple parametric models are not going to get anyone very far.\nIndeed, Breiman believes that classical research has run up against the limits of such methodologies3 due to the preponderance of larger and more complicated data sets that have become available since the digital age (Breiman 2001, 6, 8). Responding to the (traditional statisticians) hammer and (parametrically subsumable) nail analogy, he notes: “The trouble for statisticians is that recently some of the problems have stopped looking like nails.” (2001, 6). He vouches for a range of modern, computationally powerful tools as providing a path forward that he subsumes under the nomenclature of the “Algorithmic Modeling Culture”, AMC, as opposed to consigning traditional academics to a “Data Modeling Culture” or DMC (2001, 1).\nPoignantly, Breiman holds that the best models are those that provide the highest predictive accuracy, such as neural nets, random trees (or ‘forests’ composed of them), and support vector algorithms (2001, 9-11). He notes that despite their impenetrability to easy (or any) understanding, such ‘black box’ processes are to be preferred to more traditional (parametric) ones because “The goal is not interpretability, but accurate information.” (Breiman 2001, 12). To bring the point home, in response to traditional objections to using such complicated algorithmic tools because they do not lend themselves to interpretability, he notes the “…wrong question is being asked.” (2001, 11)\nIn contradistinction, Galit Shmueli focuses not on the merits of DMC or AMC cultures per se but more on the rationale for the different enterprises of predicting new data and explaining currently existing data sets and the resulting impact of an asymmetrical focus on their distinction (Shmueli 2010, 1, 16). Shmueli organizes his thesis around an axis of bifurcated aspects of modes of ‘doing statistical analysis’: ‘Causation–Association’, ‘Theory–Data’, ‘Retrospective–Prospective’, and ‘Bias–Variance’ (2010, 5). Explanatory modelling and predicative analysis4 are conceptually situated opposite to each other in every pair of this scheme, in order to reveal their fundamental differences of modality (2010, 5).\nFor starters, explanatory modelling is supposed to specify all the relevant variables that are assumed to cause an outcome(s) of interest, as well as undergird the causal pathways as of to how the variables cause those outcome(s) (Shmueli 2010, 2). He notes that in many disciplines, mere association-based tools5 such as regression analysis are used on data for “testing causal hypotheses about theoretical constructs” (2010, 2-3). Meaning that ‘extra-modelled’ inferences must buttress the theory.6 Therefore, following his axis paradigm in explanatory modeling (2010, 5):\n1) Causation channels are supposed to be captured by some sort of statistical model from which to draw conclusions of ontological significance (an act of inference)\n2) This model is often constructed with theoretical considerations in mind\n3) The focus is on data that already exists and\n4) It focuses on minimizing bias in order to properly undercover the magnitude of various variables.\nThe intrinsic value of such an approach is, therefore, assumed to lie in its ability to cohere data into a story the human mind can appreciate. As theoretical physicist David Deutsch remarks in the conclusion to the quote that opened this article:\nPredictive modelling, on the other hand, via the same axis is (Shmueli 2010, 5, 11):\n1) Concerned primarily with associations, whatever their real underlying relationship\n2) Estimates a model drawn from the data and not independently from it\n3) Is focused on future, currently non-existent data and\n4) Is willing to incur bias if need be in order to improve estimation.\nIn concurrence with Breiman, Shmueli mentions that predictive modelling is able to take advantage of a “…range of plausible methods [that]7 includes not only statistical models (interpretable and uninterpretable) but also data mining algorithms.” (2010, 10). Even if those algorithms “… are considered ill-suited for explanatory modeling.” (2010, 10).\nWhere Shmueli shines a spotlight, however, concerns the fact that an explicit recognition of the distinction in conditions required for the different types of analysis has not always been appreciated in the literature (Shmueli 2010, 6, 16). A state of affairs that has led to an arguable loss to scientific progress, not only due to matters of epistemological rectitude but because the contributions that algorithmic/predictive modelling could make to explanatory modelling has gone underappreciated (2010, 16-18). Shmueli pleads that “by producing high-accuracy price predictions they shed light on new potential variables that are related to…the types of relationships that can be further investigated in terms of causality.” (2010, 16). In contrast to Breiman’s championing of AMC over DMC practices (Breiman 2001, 16), Shmueli sees a role for the ‘two cultures’ to work together - although, to reiterate, he is also convinced that traditional statistical practice has harmed itself by not more proactively embracing the new tools provided by predictive algorithms (Shmueli 2010, 16).\nIn the conciliatory respect, Shmueli has company with Adel Daoud and Devdatt Dubhashi, whose article Statistical Modeling: The Three Cultures acts as a partial rejoinder to Breiman’s and advocates a path not dissimilar to that Shmueli does. They argue that Breiman does not provide an argument as of to why predication advances scientific progress (Daoud and Dubhashi 2023, 30) and aim to provide their own counterargument that ends up aligning with some of the aforementioned positions staked out by Shmueli (Shmueli 2010, 18–22, 24–27, 32). Daoud and Dubhashi argue that rather than best considered as antagonists to explanatory frameworks at the other end of a hostile spectrum,8 that the methods of algorithmic learning models have developed a synergetic coupling with explanatory modelling among some researchers (2023, 17). This has led to the formation of what they term a hybrid modelling culture, or HMC (2023, 3, 5, 17).\nThey reason this culture has flourished because it can offer a new alley-way for traditional causal frameworks9 to expand the range of potential pathways that can be brought into their service via machine-learning/prediction algorithms (2023, 5, 33-34). While they point out that isolated approaches of either vein are useful for certain problems (2023, 16, 27, 32), and HMC type approaches come with their own potential problems (34), they believe that the most lucrative direction for the applied social sciences lies in utilizing the strength of algorithmic modelling to better complement the goal usually left to more traditional statistical inference (35).\nTraditional inference concerns such as endogeneity (Shmueli 2010, 9–10) and the struggle to ‘operationalize’ estimators of population parameters (2010, 2, 14-15, 19) will likely still occupy researchers time even if adoption of learning-based algorithms becomes widespread. However, it seems fair to concede that problems related to systemic errors (Daoud and Dubhashi 2023, 27–18) on the scales only possible to encounter with ever larger data sets will probably be better solved by aiding the ‘mind’s eye’ with a large number of relatively blind but perhaps further seeing algorithmic ones.\nLooking towards an ever more data-rich world and very a human desire to understand it, let us hope the words of one researcher are heeded:"
    },
    {
      "objectID": "Assignment1.html#footnotes",
      "href": "Assignment1.html#footnotes",
      "title": "Assignment 1",
      "section": "Footnotes",
      "text": "Footnotes\n\n\nAs well as because truly verifiable, measurable, randomized introductions of exogenous variables (experiments) are hard to come by ‘in the wild’, especially in the social sciences (see footnote 6), and causal perturbation effects need to be identified by accounting (‘controlling’) for possible missing/hidden variables (Huntington-Klein 2022, ch. 4, 8-11, passim). A task that can therefore be aided considerably if it can be assumed the underlying relationships are described by a well-defined parametric model.↩︎\nEssentially, a model whose structure is presumed in advance, hence making for a more simplified grasp of reality (Hoskin n.d.).↩︎\nThese limits are arguably always there anyway, in a sense that has been recognized for centuries: “But philosophers, who carry their scrutiny a little farther, immediately perceive, that, even in the most familiar events, the energy of the cause is as unintelligible as in the most unusual, and that we only learn by experience the frequent CONJUNCTION of objects, without being ever able to comprehend any thing like CONNEXION between them.” - (Hume et al. 1993, 52)↩︎\nHe also focuses on purely descriptive analysis, see (2010, 3).↩︎\n“Whereas “proper” statistical methodology for testing causality exists, such as designed experiments or specialized causal inference methods for observational data…” (2010, 3).↩︎\n“In complex situations—and most social science is about highly complex situations—we may well not know very much about what the causal…looks like.” - (Huntington-Klein 2022, 161)↩︎\nItalics mine.↩︎\nIt is important to note that Shmueli also sees the antagonism as fairly historic in nature and views the two approaches as more of a set of potentially complimentary dimensions (2010, 17).↩︎\nThey take the ‘hypothetico-deductive’ method as the central one (Daoud and Dubhashi 2023, 5)↩︎"
    },
    {
      "objectID": "Ass 3.html",
      "href": "Ass 3.html",
      "title": "Assignment 3",
      "section": "",
      "text": "Loading in the ‘TEDS_2016’ data-set, I attempt to run a regression on ‘Tondu’, selectively using ‘Age’, ‘Edu’, and ‘income’ as predictor variables. To do this, I use a command to create an function labeled ‘regplot’:\n\nlibrary(tidyverse)\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nattach(TEDS_2016)\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}\nregplot(Tondu,age,xlab=\"age\",ylab=\"Tondu\",col=\"blue\",pch=20)\n\n\n\n\nNext, I cycle through each iteration with permutations of the following command in order to create and graph the regressions:\n\nlibrary(tidyverse)\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nattach(TEDS_2016)\nregplot(Tondu,income)\n\n\n\nregplot(Tondu,Age)\n\n\n\nregplot(Tondu,Edu)\n\n\n\n\nBut I would like to get a more comprehensive picture of the multivariate relationships I am dealing with. To try to better grapple with these, I construct a 3d plot, regressing ‘Tondu’ on both ‘income’ and ‘edu’ using:\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nTEDS_2016$Tondu &lt;- as.numeric(as.character(TEDS_2016$Tondu))\nlibrary(tidyverse)\nlibrary(plotly)\nattach(TEDS_2016)\nTEDS_2016_plot &lt;- plot_ly(TEDS_2016,\n                     x = income,\n                     y = edu,\n                     z = Tondu,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\nTEDS_2016_plot\n\n\n\n\n\nAn impressive visual, to say the least. But this relationship is not necessarily any clearer. What to do? Well, because I’ve used the variable dictionary within Rstudio, I can see that ‘Tondu’ really is a categorical variable. In other words, it is non-hierarchically ordered qualitative response predictor. Is there any sort of regression that can handle categorical dependent variables? There is: those that are known as ‘multinominal logit regressions’. Basically, a special type of log twist of standard OLS regression."
    },
    {
      "objectID": "Ass 3.html#a-first-attempt",
      "href": "Ass 3.html#a-first-attempt",
      "title": "Assignment 3",
      "section": "",
      "text": "Loading in the ‘TEDS_2016’ data-set, I attempt to run a regression on ‘Tondu’, selectively using ‘Age’, ‘Edu’, and ‘income’ as predictor variables. To do this, I use a command to create an function labeled ‘regplot’:\n\nlibrary(tidyverse)\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nattach(TEDS_2016)\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}\nregplot(Tondu,age,xlab=\"age\",ylab=\"Tondu\",col=\"blue\",pch=20)\n\n\n\n\nNext, I cycle through each iteration with permutations of the following command in order to create and graph the regressions:\n\nlibrary(tidyverse)\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nattach(TEDS_2016)\nregplot(Tondu,income)\n\n\n\nregplot(Tondu,Age)\n\n\n\nregplot(Tondu,Edu)\n\n\n\n\nBut I would like to get a more comprehensive picture of the multivariate relationships I am dealing with. To try to better grapple with these, I construct a 3d plot, regressing ‘Tondu’ on both ‘income’ and ‘edu’ using:\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nTEDS_2016$Tondu &lt;- as.numeric(as.character(TEDS_2016$Tondu))\nlibrary(tidyverse)\nlibrary(plotly)\nattach(TEDS_2016)\nTEDS_2016_plot &lt;- plot_ly(TEDS_2016,\n                     x = income,\n                     y = edu,\n                     z = Tondu,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\nTEDS_2016_plot\n\n\n\n\n\nAn impressive visual, to say the least. But this relationship is not necessarily any clearer. What to do? Well, because I’ve used the variable dictionary within Rstudio, I can see that ‘Tondu’ really is a categorical variable. In other words, it is non-hierarchically ordered qualitative response predictor. Is there any sort of regression that can handle categorical dependent variables? There is: those that are known as ‘multinominal logit regressions’. Basically, a special type of log twist of standard OLS regression."
    },
    {
      "objectID": "Ass 3.html#getting-logisticical",
      "href": "Ass 3.html#getting-logisticical",
      "title": "Assignment 3",
      "section": "Getting Log(istic)ical",
      "text": "Getting Log(istic)ical\n\nAs a first step, I convert ‘Tondu’ into a proper categorical variable with the ‘factor’ function:\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nTEDS_2016=na.omit(TEDS_2016)\nattach(TEDS_2016)\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels = c(1,2,3,4,5, 6, 9), labels = c(\"Unification now\", \"Status quo, unif. in future\", \"Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now\", \"No response\")) \n\nThis also attaches the requite labels to the output levels of the variable. Now, MLRs have alot of fancy theory behind them which you can check up on here. I will be utilizing the modeling techniques described here. One good thing about MLRs is that we can treat explanatory ordinally structured variables as continuous.\nSo, proceeding ahead I make my MLR model with the ‘nnet’ library function ‘multinom’, and then run a summary:\n\nlibrary(nnet)\nTEDDY &lt;- multinom(Tondu ~ Age + Edu + income, data = TEDS_2016,\n                  Hess = TRUE, trace = FALSE)\nsummary(TEDDY, Wald.ratios = TRUE)\n\nCall:\nmultinom(formula = Tondu ~ Age + Edu + income, data = TEDS_2016, \n    Hess = TRUE, trace = FALSE)\n\nCoefficients:\n                             (Intercept)         Age         Edu       income\nStatus quo, unif. in future    -1.279791  0.32960034  0.57144475  0.073627701\nStatus quo, decide later        1.612667 -0.11181363  0.49000077  0.058884230\nStatus quo forever              1.031011  0.05532192  0.36193152  0.037564845\nStatus quo, indep. in future    1.640629 -0.27243191  0.53290606  0.063371340\nIndependence now                2.588714 -0.38408305  0.08254578 -0.008191805\nNo response                     0.525247  0.38905103 -0.64611163 -0.190700335\n\nStd. Errors:\n                             (Intercept)       Age       Edu     income\nStatus quo, unif. in future     1.516283 0.2671854 0.2444852 0.08959741\nStatus quo, decide later        1.462681 0.2584432 0.2382903 0.08657418\nStatus quo forever              1.483784 0.2622312 0.2409906 0.08771021\nStatus quo, indep. in future    1.473995 0.2600440 0.2410328 0.08761416\nIndependence now                1.569008 0.2774157 0.2595938 0.09466307\nNo response                     2.344280 0.4250818 0.3830006 0.12245914\n\nValue/SE (Wald statistics):\n                             (Intercept)        Age        Edu      income\nStatus quo, unif. in future   -0.8440320  1.2336018  2.3373390  0.82176148\nStatus quo, decide later       1.1025420 -0.4326430  2.0563190  0.68015925\nStatus quo forever             0.6948523  0.2109662  1.5018494  0.42828361\nStatus quo, indep. in future   1.1130494 -1.0476376  2.2109276  0.72330019\nIndependence now               1.6499050 -1.3845035  0.3179806 -0.08653644\nNo response                    0.2240547  0.9152380 -1.6869729 -1.55725685\n\nResidual Deviance: 3319.504 \nAIC: 3367.504 \n\n\nNext, in order to make these results more amenable to probabilistic interpretation, I use the ‘predict’ function from the ‘car’ library:\n\npredict(TEDDY, newdata = data.frame(Age = 1, Edu = 1, income = 1), type = 'probs')\n\n             Unification now  Status quo, unif. in future \n                  0.03134793                   0.02310456 \n    Status quo, decide later           Status quo forever \n                  0.24344397                   0.13851481 \nStatus quo, indep. in future             Independence now \n                  0.22354726                   0.30616791 \n                 No response \n                  0.03387355 \n\n\nWhich means that when ‘Age’ = ‘Edu’ = ‘income’ = 1, the probability of supporting ‘Unification now’ goes us about 3.1%, supporting ‘Status quo forever’ goes up about 14%, etc.\nA better way to get a wider probabilistic output is to use the ‘ggeffects’ library (note the other variables are specified according to their level-schemes: e.g. 1-5 with ‘base’ 1 in the case of ‘Age’, etc.):\n#| warning: false\nlibrary(ggeffects)\nggeffect(TEDDY, terms = \"Age[1:5,by=1]\") \nggeffect(TEDDY, terms = \"Edu[1:5,by=1]\")\nggeffect(TEDDY, terms = \"income[1:10,by=1]\")\nGiving me the following output:\n\nWhich gives us the estimated probability of supporting, say, ‘Status quo, independence in the future’ for age bracket ‘1’ = 0.36 or 36%.\nFinally, I will graph the probability outputs just derived (saving them as mark1, mark2, etc.) against the explanatory variables using variations of:\nggplot(mark1) +\n  aes(x = x, y = predicted, fill = response.level, color = response.level) +\n  geom_line() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 1/3) +\n  labs(x = 'Age', y = 'Predicted Probability') +\n  ylim(c(0,1))\nWhich gives me the following outputs:\n\n\n\n  \n\n\n\nShowing the effect plots of levels of the explanatory variables associated with the probabilities of certain levels of ‘Tondu’."
    },
    {
      "objectID": "Ass 2.html",
      "href": "Ass 2.html",
      "title": "Assignment 2",
      "section": "",
      "text": "To begin this assignment, I loaded the requisite data set, ‘TED2016’ into R studio (after initiating the ‘tidyverse’ and ‘haven’ packages) with the following code:   \n\nlibrary(haven)\nTEDS_2016 &lt;- haven::read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nNaming the dataset as ‘TEDS_2016’. To get a preliminary ‘hold’ of the data, I go ahead and clean the variable names using:\n\nTEDS_2016 |&gt; janitor::clean_names()\n\n# A tibble: 1,690 × 54\n   district     sex     age     edu     arear   career  career8 ethnic  party   \n   &lt;dbl+lbl&gt;    &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n 1 201 [Yi Lan… 2 [Fem… 4 [50-… 4 [Col… 1 [Tai… 1 [Hig… 1 [Civ… 1 [Tai… 25 [Neu…\n 2 201 [Yi Lan… 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n 3 201 [Yi Lan… 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n 4 201 [Yi Lan… 1 [Mal… 4 [50-… 2 [Jun… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n 5 201 [Yi Lan… 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 9 [Nor… 25 [Neu…\n 6 201 [Yi Lan… 2 [Fem… 5 [Abo… 2 [Jun… 1 [Tai… 2 [Low… 7 [Hou… 1 [Tai…  6 [Som…\n 7 201 [Yi Lan… 1 [Mal… 5 [Abo… 1 [Bel… 1 [Tai… 4 [WOR… 4 [Lab… 2 [Bot… 25 [Neu…\n 8 201 [Yi Lan… 2 [Fem… 4 [50-… 5 [Abo… 1 [Tai… 1 [Hig… 2 [Man… 1 [Tai… 24 [Som…\n 9 201 [Yi Lan… 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n10 201 [Yi Lan… 1 [Mal… 4 [50-… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 2 [Bot… 25 [Neu…\n# ℹ 1,680 more rows\n# ℹ 45 more variables: party_id &lt;dbl+lbl&gt;, tondu &lt;dbl+lbl&gt;, tondu3 &lt;dbl+lbl&gt;,\n#   n_i2 &lt;dbl+lbl&gt;, votetsai &lt;dbl&gt;, green &lt;dbl&gt;, votetsai_nm &lt;dbl&gt;,\n#   votetsai_all &lt;dbl&gt;, independence &lt;dbl&gt;, unification &lt;dbl&gt;, sq &lt;dbl&gt;,\n#   taiwanese &lt;dbl&gt;, edu_2 &lt;dbl&gt;, female &lt;dbl&gt;, whitecollar &lt;dbl&gt;,\n#   lowincome &lt;dbl&gt;, income &lt;dbl&gt;, income_nm &lt;dbl&gt;, age_2 &lt;dbl&gt;, kmt &lt;dbl&gt;,\n#   dpp &lt;dbl&gt;, npp &lt;dbl&gt;, noparty &lt;dbl&gt;, pfp &lt;dbl&gt;, south &lt;dbl&gt;, north &lt;dbl&gt;, …\n\n\nI then take a look at the entire variable (column) list. I notice immediately that some of the 1,690 observations are missing data points, ‘NA’. I figure my first step should be to deal with these before proceeding too far along. But also, some of the variables appear to be categorical or binary. A fact I can at least cursorily confirm by opening the visualization extension in RStudio’s ‘Environment’ pane. I also note that some of the columns might be combing information from other variables.\nA missing value can be alikened to a piece of reality that refuses to show up. You figure it must be there, so one is tempted to ignore or rationalize for it, yet it may get in the way of being able to deal with what is at least available.\nI run:\n\nsum(is.na(TEDS_2016))\n\n[1] 3008\n\n\nWhich tells me that I am missing 3008 cell values in total.\nSo, for the moment I used the following dplyr command to remove missing values:\n\nTEDS_2016=na.omit(TEDS_2016)\n\nThis may pose a risk of introducing basis in my analysis from this point forward. Indeed, this move reduces the number of rows by 626, or about 37%. However, in the interests of data integrity, I decide to proceed with the removal.\n\n\n\n\nIn order to compare the variable ‘Tondu’ with the variables ‘female’, ‘DPP’, ‘age’, ‘income’, ‘edu’, ‘Taiwanese’ and ‘Econ_worse’, I utilize the ggplot command ‘geom_bar’ in order to get a visual representation of the relationship between the variables.\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels = c(1,2,3,4,5, 6, 9), labels = c(\"Unification now\", \"Status quo, unif. in future\", \"Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now\", \"No response\"))\npairs(~ Tondu + income + edu + Taiwanese + Econ_worse + age + DPP + female, TEDS_2016)\n\n\n\n\nNote that I first turn ‘Tondu’ into a factor variable with a response class consisting of:“Unification now”, “Status quo, unif. in future”, “Status quo, decide later”, “Status quo forever”, “Status quo, indep. in future”, “Independence now”, and “No response”.\nWe can see some of the concentration between different sub-sections of some of the prominent predictors and the output variable ‘Tondu’.\nWe can also compare the ‘votesai’ variable with other independent variables:\n\nTEDS_2016$votesai &lt;- factor(TEDS_2016$votetsai, levels = c(1,2), labels = c(\"Yes\", \"No\"))\npairs(~ votetsai + income + edu + Taiwanese + Econ_worse + age + DPP + female, TEDS_2016)\n\n\n\n\nWe can also simply plot the amount of respondents who expressed support for Tsai:\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(votetsai)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nOf course at this stage, we are just playing with the data. We do not yet know for sure whether or not our classifications are appropriate.\n\n\n\n\n\n\nTo further analyze this selection of the data-set in relation to various other key explanatory variables, I make some logit regressions and plot them as so (see Assignment 3 for details):\n\n\n\n  \n\n\n\nFinally, I plot a frequency chart of ‘Tondu’:\n\nlibrary(descr)\nfreq(TEDS_2016$Tondu)\n\n\n\n\nTEDS_2016$Tondu \n                             Frequency Percent\nUnification now                     18   1.676\nStatus quo, unif. in future        131  12.197\nStatus quo, decide later           351  32.682\nStatus quo forever                 200  18.622\nStatus quo, indep. in future       270  25.140\nIndependence now                    73   6.797\nNo response                         31   2.886\nTotal                             1074 100.000\n\n\nWhich roughly shows us the concentration of this variable. Indicating that the most frequent response chosen is ‘Status quo, decide later’."
    },
    {
      "objectID": "Ass 2.html.html#initialization",
      "href": "Ass 2.html.html#initialization",
      "title": "Assignment 2",
      "section": "",
      "text": "To begin this assignment, I loaded the requisite data set, ‘TED2016’ into R studio (after initiating the ‘tidyverse’ and ‘haven’ packages) with the following code:   \n\nlibrary(haven)\nTEDS_2016 &lt;- haven::read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nNaming the dataset as ‘TEDS_2016’. To get a preliminary ‘hold’ of the data, I go ahead and clean the variable names using:\n\nTEDS_2016 |&gt; janitor::clean_names()\n\n# A tibble: 1,690 × 54\n   district     sex     age     edu     arear   career  career8 ethnic  party   \n   &lt;dbl+lbl&gt;    &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n 1 201 [Yi Lan… 2 [Fem… 4 [50-… 4 [Col… 1 [Tai… 1 [Hig… 1 [Civ… 1 [Tai… 25 [Neu…\n 2 201 [Yi Lan… 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n 3 201 [Yi Lan… 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n 4 201 [Yi Lan… 1 [Mal… 4 [50-… 2 [Jun… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n 5 201 [Yi Lan… 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 9 [Nor… 25 [Neu…\n 6 201 [Yi Lan… 2 [Fem… 5 [Abo… 2 [Jun… 1 [Tai… 2 [Low… 7 [Hou… 1 [Tai…  6 [Som…\n 7 201 [Yi Lan… 1 [Mal… 5 [Abo… 1 [Bel… 1 [Tai… 4 [WOR… 4 [Lab… 2 [Bot… 25 [Neu…\n 8 201 [Yi Lan… 2 [Fem… 4 [50-… 5 [Abo… 1 [Tai… 1 [Hig… 2 [Man… 1 [Tai… 24 [Som…\n 9 201 [Yi Lan… 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n10 201 [Yi Lan… 1 [Mal… 4 [50-… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 2 [Bot… 25 [Neu…\n# ℹ 1,680 more rows\n# ℹ 45 more variables: party_id &lt;dbl+lbl&gt;, tondu &lt;dbl+lbl&gt;, tondu3 &lt;dbl+lbl&gt;,\n#   n_i2 &lt;dbl+lbl&gt;, votetsai &lt;dbl&gt;, green &lt;dbl&gt;, votetsai_nm &lt;dbl&gt;,\n#   votetsai_all &lt;dbl&gt;, independence &lt;dbl&gt;, unification &lt;dbl&gt;, sq &lt;dbl&gt;,\n#   taiwanese &lt;dbl&gt;, edu_2 &lt;dbl&gt;, female &lt;dbl&gt;, whitecollar &lt;dbl&gt;,\n#   lowincome &lt;dbl&gt;, income &lt;dbl&gt;, income_nm &lt;dbl&gt;, age_2 &lt;dbl&gt;, kmt &lt;dbl&gt;,\n#   dpp &lt;dbl&gt;, npp &lt;dbl&gt;, noparty &lt;dbl&gt;, pfp &lt;dbl&gt;, south &lt;dbl&gt;, north &lt;dbl&gt;, …\n\n\nI then take a look at the entire variable (column) list. I notice immediately that some of the 1,690 observations are missing data points, ‘NA’. I figure my first step should be to deal with these before proceeding too far along. But also, some of the variables appear to be categorical or binary. A fact I can at least cursorily confirm by opening the visualization extension in RStudio’s ‘Environment’ pane. I also note that some of the columns might be combing information from other variables.\nA missing value can be alikened to a piece of reality that refuses to show up. You figure it must be there, so one is tempted to ignore or rationalize for it, yet it may get in the way of being able to deal with what is at least available.\nI run:\n\nsum(is.na(TEDS_2016))\n\n[1] 3008\n\n\nWhich tells me that I am missing 3008 cell values in total.\nSo, for the moment I used the following dplyr command to remove missing values:\n\nTEDS_2016=na.omit(TEDS_2016)\n\nThis may pose a risk of introducing basis in my analysis from this point forward. Indeed, this move reduces the number of rows by 626, or about 37%. However, in the interests of data integrity, I decide to proceed with the removal."
    },
    {
      "objectID": "Ass 2.html#comparing-variables-with-tondu",
      "href": "Ass 2.html#comparing-variables-with-tondu",
      "title": "Assignment 2",
      "section": "",
      "text": "In order to compare the variable ‘Tondu’ with the variables ‘female’, ‘DPP’, ‘age’, ‘income’, ‘edu’, ‘Taiwanese’ and ‘Econ_worse’, I utilize the ggplot command ‘geom_bar’ in order to get a visual representation of the relationship between the variables.\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels = c(1,2,3,4,5, 6, 9), labels = c(\"Unification now\", \"Status quo, unif. in future\", \"Status quo, decide later\", \"Status quo forever\", \"Status quo, indep. in future\", \"Independence now\", \"No response\"))\npairs(~ Tondu + income + edu + Taiwanese + Econ_worse + age + DPP + female, TEDS_2016)\n\n\n\n\nNote that I first turn ‘Tondu’ into a factor variable with a response class consisting of:“Unification now”, “Status quo, unif. in future”, “Status quo, decide later”, “Status quo forever”, “Status quo, indep. in future”, “Independence now”, and “No response”.\nWe can see some of the concentration between different sub-sections of some of the prominent predictors and the output variable ‘Tondu’.\nWe can also compare the ‘votesai’ variable with other independent variables:\n\nTEDS_2016$votesai &lt;- factor(TEDS_2016$votetsai, levels = c(1,2), labels = c(\"Yes\", \"No\"))\npairs(~ votetsai + income + edu + Taiwanese + Econ_worse + age + DPP + female, TEDS_2016)\n\n\n\n\nWe can also simply plot the amount of respondents who expressed support for Tsai:\n\nlibrary(ggplot2)\nggplot(TEDS_2016, aes(votetsai)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nOf course at this stage, we are just playing with the data. We do not yet know for sure whether or not our classifications are appropriate.\n\n\n\n\n\n\nTo further analyze this selection of the data-set in relation to various other key explanatory variables, I make some logit regressions and plot them as so (see Assignment 3 for details):\n\n\n\n  \n\n\n\nFinally, I plot a frequency chart of ‘Tondu’:\n\nlibrary(descr)\nfreq(TEDS_2016$Tondu)\n\n\n\n\nTEDS_2016$Tondu \n                             Frequency Percent\nUnification now                     18   1.676\nStatus quo, unif. in future        131  12.197\nStatus quo, decide later           351  32.682\nStatus quo forever                 200  18.622\nStatus quo, indep. in future       270  25.140\nIndependence now                    73   6.797\nNo response                         31   2.886\nTotal                             1074 100.000\n\n\nWhich roughly shows us the concentration of this variable. Indicating that the most frequent response chosen is ‘Status quo, decide later’."
    },
    {
      "objectID": "Assignment 7.html",
      "href": "Assignment 7.html",
      "title": "Assignment 7",
      "section": "",
      "text": "Sometimes, we want to classify observations more accurately than we can using standard logistical regression. One way to do this is via Linear Discriminant Analysis (LDA). In cases where the categorical classes are widely different, for example, LDA can outperform logistical regression in correctly assigning observations to their correct class. LDA is able to beat-out logistical regression because it is essentially a linear estimator of of the Bayes classifier, which is theoretically the ‘optimal’ classifier.\nOf course, LDA has to make assumptions in order to estimate Bayesian decision boundaries. Namely these involve things regarding the distribution from which errors come from. Although in practice, such rules can be broken and analysis can proceed anyway.\nOf course, a method like this can possibly suffer from more fundamental drawbacks. Because LDA is a binary classifier (e.g. something is either assigned to a class or it is not), it can:\n\npotentially correctly identify an observation as belonging to the class it does, an metric measured as sensitivity.\nor\nit can correctly identify that the observations that do not belong to a class in fact do not belong to it, a metric measured as specificity.\n\nThe drawbacks come in because of the particular way LDA makes its estimation of the Bayesian decision boundaries. By ‘mimicking’ Bayes, LDA is trying to reduce the overall classification error rate. Indeed, the possibly of problems arising because of this becomes stark when it is observed that for some data-sets, say one where 99.99% of all observations are all members of single class, a ‘dumb’ classifier that automatically assigns all observations to that class would also have a pretty low overall error rate.\nOne way to represent this possible trade-off between different types of error rates is via an ROC curve, like the one here:\n\nThe best classification procedure will result in an output where the blue line basically hugs the upper left corner.\nAnother way is via a confusion table/matrix, which typically takes the form of such a table as the one below:\n\nTo illustrate their usage, we can see below that of the 333 individuals who ‘default’, 81/333 = 26.43% were correctly identified who did so. While 9644/9667 = 99.76% of those who did did were correctly identified.\n\nThe question as of to whether sensitivity or specificity is more important depends upon the question we want answered. If it is more costly to misidentify someone as not belonging to a certain class when to proportion of individuals who fit that description is small in our data-set, we may trade-off a higher total error rate to gain some competence in regards to specificity. It is also possible to modify LDA to help deal with such cases, though that is a story which will have to wait for both us and another time to find ourselves in."
    },
    {
      "objectID": "index.html",
      "href": "index.html",
      "title": "Santoshi Rishitha",
      "section": "",
      "text": ""
    },
    {
      "objectID": "Assignment 8.html",
      "href": "Assignment 8.html",
      "title": "Assignment 8",
      "section": "",
      "text": "We will here preform best subset section. As a first step, we’ll do some multi-linear regression via:\n\nset.seed(1)\nX = rnorm(100)\neps = rnorm(100)\nB0 = 4\nB1 = 9\nB2 = 2\nB3 = 1\nY &lt;- B0 + B1 * X + B2 * X^2 + B3 * X^3 + eps\nplot(Y,pch=20, cex=.8, col=\"steelblue\")\n\n\n\n\nNext:\n\nlibrary(leaps)\nreggie = regsubsets(Y~poly(X,10,raw=T), data=data.frame(Y,X), nvmax=10)\nreggie.sum = summary(reggie)\n\nThen checking the Cp value:\n\nreggie.sum$cp\n\n [1] 873.3368122 223.7534690   2.1859433   0.6067483   2.1782005   3.9955812\n [7]   5.7869063   7.1694092   9.0016690  11.0000000\n\n\nAdd in BIC:\n\nreggie.sum$bic\n\n [1] -247.1841 -354.1125 -471.1079 -470.3769 -466.2458 -461.8434 -457.4704\n [8] -453.5553 -449.1384 -444.5351\n\n\nPlus Adj R^2:\n\nreggie.sum$adjr2\n\n [1] 0.9222139 0.9742384 0.9922844 0.9924995 0.9924556 0.9923899 0.9923250\n [8] 0.9922940 0.9922231 0.9921358\n\n\nVisually:\n\npar(mfow=c(1,3))\nplot(reggie.sum$cp)\n\n\n\nplot(reggie.sum$bic)\n\n\n\nplot(reggie.sum$adjr2)\n\n\n\n\nAccording to Cp criteria, model ‘4’ is best. Via BIC, model ‘4’ also wins out. Adj-R^2 also seems to attribute no more worth to any model than to the fourth one.\nRerunning this selection process for forward step-wise selection:\n\nlibrary(leaps)\nreggie2 = regsubsets(Y~poly(X,10,raw=T), method = \"forward\", data=data.frame(Y,X), nvmax=10)\nreggie.sum2 = summary(reggie2)\n\n\nreggie.sum2$cp\n\n [1] 873.336812 223.753469  45.153233   2.512810   2.193128   4.124398\n [7]   6.036511   7.968099   9.957999  11.000000\n\n\nthen:\n\nreggie.sum2$bic\n\n [1] -247.1841 -354.1125 -433.5251 -468.2951 -466.2293 -461.7004 -457.1928\n [8] -452.6636 -448.0697 -444.5351\n\n\nand:\n\nreggie.sum2$adjr2\n\n [1] 0.9222139 0.9742384 0.9887646 0.9923417 0.9924543 0.9923790 0.9923037\n [8] 0.9922250 0.9921395 0.9921358\n\n\nwith:\n\npar(mfow=c(1,3))\nplot(reggie.sum2$cp)\n\n\n\nplot(reggie.sum2$bic)\n\n\n\nplot(reggie.sum2$adjr2)\n\n\n\n\nIt looks like ‘4’ is still the winner. Let us try backwards-step-wise selection:\n\nlibrary(leaps)\nreggie3 = regsubsets(Y~poly(X,10,raw=T), method = \"backward\", data=data.frame(Y,X), nvmax=10)\nreggie.sum3 = summary(reggie2)\n\nnext:\n\nreggie.sum3$cp\n\n [1] 873.336812 223.753469  45.153233   2.512810   2.193128   4.124398\n [7]   6.036511   7.968099   9.957999  11.000000\n\n\nthen:\n\nreggie.sum3$bic\n\n [1] -247.1841 -354.1125 -433.5251 -468.2951 -466.2293 -461.7004 -457.1928\n [8] -452.6636 -448.0697 -444.5351\n\n\nand:\n\nreggie.sum3$bic\n\n [1] -247.1841 -354.1125 -433.5251 -468.2951 -466.2293 -461.7004 -457.1928\n [8] -452.6636 -448.0697 -444.5351\n\n\nvisually:\n\npar(mfow=c(1,3))\nplot(reggie.sum3$cp)\n\n\n\nplot(reggie.sum3$bic)\n\n\n\nplot(reggie.sum3$adjr2)\n\n\n\n\n‘4’ still seems to take it home!"
    }
  ]